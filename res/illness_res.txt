ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
Gradient Checkpointing: True
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
[2025-03-31 18:51:28,121] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-31 18:51:28,586] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-03-31 18:51:28,586] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-03-31 18:51:28,586] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-03-31 18:51:29,260] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.51.187, master_port=29500
[2025-03-31 18:51:29,261] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-31 18:51:38,065] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-31 18:51:38,066] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-03-31 18:51:38,067] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-31 18:51:38,067] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-03-31 18:51:38,067] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-03-31 18:51:38,067] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-31 18:51:38,068] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-31 18:51:38,068] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-31 18:51:38,068] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-31 18:51:38,068] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-31 18:51:38,290] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2025-03-31 18:51:38,291] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.6 GB         CA 12.61 GB         Max_CA 13 GB 
[2025-03-31 18:51:38,291] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.08 GB, percent = 1.1%
[2025-03-31 18:51:38,396] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2025-03-31 18:51:38,396] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.67 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 18:51:38,396] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.08 GB, percent = 1.1%
[2025-03-31 18:51:38,396] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2025-03-31 18:51:38,498] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2025-03-31 18:51:38,499] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.54 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 18:51:38,499] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.08 GB, percent = 1.1%
[2025-03-31 18:51:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2025-03-31 18:51:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-03-31 18:51:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-03-31 18:51:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]
[2025-03-31 18:51:38,500] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-03-31 18:51:38,500] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-31 18:51:38,500] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-31 18:51:38,500] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-03-31 18:51:38,500] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3500332910>
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-03-31 18:51:38,501] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-31 18:51:38,502] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   world_size ................... 1
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-31 18:51:38,503] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2025-03-31 18:51:38,503] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.8785348
	speed: 1.7676s/iter; left time: 2069.8214s
Epoch: 1 cost time: 212.13904190063477
Epoch: 1 | Train Loss: 0.8469058 Vali Loss: 0.3324744 Test Loss: 2.2261856 MAE Loss: 0.9868010
lr = 0.0004000000
Updating learning rate to 0.0003999999999999993
	iters: 100, epoch: 2 | loss: 0.2455960
	speed: 2.3974s/iter; left time: 2502.9342s
Epoch: 2 cost time: 210.571031332016
Epoch: 2 | Train Loss: 0.5344221 Vali Loss: 0.3746994 Test Loss: 2.3649169 MAE Loss: 1.0491138
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00019999999999999966
	iters: 100, epoch: 3 | loss: 0.4339066
	speed: 2.3942s/iter; left time: 2195.4703s
Epoch: 3 cost time: 210.3420832157135
Epoch: 3 | Train Loss: 0.4410987 Vali Loss: 0.2962770 Test Loss: 2.0979375 MAE Loss: 0.9784819
Updating learning rate to 9.999999999999983e-05
	iters: 100, epoch: 4 | loss: 0.6551839
	speed: 2.3944s/iter; left time: 1891.6053s
Epoch: 4 cost time: 210.39512753486633
Epoch: 4 | Train Loss: 0.3886484 Vali Loss: 0.2512747 Test Loss: 1.9027433 MAE Loss: 0.8803233
Updating learning rate to 4.9999999999999914e-05
	iters: 100, epoch: 5 | loss: 0.2683734
	speed: 2.3932s/iter; left time: 1586.6899s
Epoch: 5 cost time: 210.17229461669922
Epoch: 5 | Train Loss: 0.3591152 Vali Loss: 0.2514201 Test Loss: 1.9083300 MAE Loss: 0.8929866
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4999999999999957e-05
	iters: 100, epoch: 6 | loss: 0.3122848
	speed: 2.3935s/iter; left time: 1282.9363s
Epoch: 6 cost time: 210.2660493850708
Epoch: 6 | Train Loss: 0.3377500 Vali Loss: 0.2416687 Test Loss: 1.8547304 MAE Loss: 0.8682264
Updating learning rate to 1.2499999999999979e-05
	iters: 100, epoch: 7 | loss: 0.3131105
	speed: 2.3913s/iter; left time: 978.0329s
Epoch: 7 cost time: 210.22460627555847
Epoch: 7 | Train Loss: 0.3308958 Vali Loss: 0.2419542 Test Loss: 1.8369188 MAE Loss: 0.8603211
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.249999999999989e-06
	iters: 100, epoch: 8 | loss: 0.3241696
	speed: 2.3933s/iter; left time: 674.9243s
Epoch: 8 cost time: 210.1569972038269
Epoch: 8 | Train Loss: 0.3296103 Vali Loss: 0.2480320 Test Loss: 1.8656719 MAE Loss: 0.8720485
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1249999999999946e-06
	iters: 100, epoch: 9 | loss: 0.5169146
	speed: 2.3940s/iter; left time: 371.0697s
Epoch: 9 cost time: 210.23477911949158
Epoch: 9 | Train Loss: 0.3276183 Vali Loss: 0.2443754 Test Loss: 1.8579957 MAE Loss: 0.8675500
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5624999999999973e-06
	iters: 100, epoch: 10 | loss: 0.3161417
	speed: 2.3935s/iter; left time: 67.0167s
Epoch: 10 cost time: 210.37948632240295
Epoch: 10 | Train Loss: 0.3224673 Vali Loss: 0.2445019 Test Loss: 1.8540703 MAE Loss: 0.8677693
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.812499999999987e-07
success delete checkpoints
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
Gradient Checkpointing: True
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
[2025-03-31 19:32:23,161] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-31 19:32:23,675] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-03-31 19:32:23,675] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-03-31 19:32:23,676] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-03-31 19:32:24,410] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.51.187, master_port=29500
[2025-03-31 19:32:24,410] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-31 19:32:38,199] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-31 19:32:38,202] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-03-31 19:32:38,202] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-31 19:32:38,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-03-31 19:32:38,204] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-03-31 19:32:38,205] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-31 19:32:38,205] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-31 19:32:38,205] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-31 19:32:38,205] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-31 19:32:38,206] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-31 19:32:38,501] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2025-03-31 19:32:38,502] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.6 GB         CA 12.61 GB         Max_CA 13 GB 
[2025-03-31 19:32:38,502] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.1 GB, percent = 1.1%
[2025-03-31 19:32:38,610] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2025-03-31 19:32:38,610] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.67 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 19:32:38,611] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.1 GB, percent = 1.1%
[2025-03-31 19:32:38,611] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2025-03-31 19:32:38,716] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2025-03-31 19:32:38,717] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.54 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 19:32:38,717] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.1 GB, percent = 1.1%
[2025-03-31 19:32:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2025-03-31 19:32:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-03-31 19:32:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-03-31 19:32:38,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]
[2025-03-31 19:32:38,718] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f13301382d0>
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-03-31 19:32:38,719] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-03-31 19:32:38,720] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2025-03-31 19:32:38,721] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   world_size ................... 1
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-31 19:32:38,722] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2025-03-31 19:32:38,722] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.6838326
	speed: 1.8041s/iter; left time: 2058.4794s
Epoch: 1 cost time: 205.53969526290894
Epoch: 1 | Train Loss: 0.7443655 Vali Loss: 0.2892851 Test Loss: 2.4132545 MAE Loss: 1.0368906
lr = 0.0004000000
Updating learning rate to 0.0003999999999999993
	iters: 100, epoch: 2 | loss: 0.5590358
	speed: 2.3171s/iter; left time: 2356.4480s
Epoch: 2 cost time: 205.4844834804535
Epoch: 2 | Train Loss: 0.5028422 Vali Loss: 0.2400107 Test Loss: 2.0460434 MAE Loss: 0.9173197
Updating learning rate to 0.00019999999999999966
	iters: 100, epoch: 3 | loss: 0.3259345
	speed: 2.3162s/iter; left time: 2068.3914s
Epoch: 3 cost time: 205.3769974708557
Epoch: 3 | Train Loss: 0.4111858 Vali Loss: 0.2216532 Test Loss: 1.9335640 MAE Loss: 0.8999761
Updating learning rate to 9.999999999999983e-05
	iters: 100, epoch: 4 | loss: 0.3511001
	speed: 2.3151s/iter; left time: 1780.3026s
Epoch: 4 cost time: 205.26138305664062
Epoch: 4 | Train Loss: 0.3587330 Vali Loss: 0.2321099 Test Loss: 1.9733232 MAE Loss: 0.9025997
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9999999999999914e-05
	iters: 100, epoch: 5 | loss: 0.3487587
	speed: 2.3141s/iter; left time: 1492.5757s
Epoch: 5 cost time: 205.30816888809204
Epoch: 5 | Train Loss: 0.3323352 Vali Loss: 0.2223980 Test Loss: 2.2396743 MAE Loss: 0.9421547
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.4999999999999957e-05
	iters: 100, epoch: 6 | loss: 0.3547212
	speed: 2.3126s/iter; left time: 1204.8459s
Epoch: 6 cost time: 205.19698858261108
Epoch: 6 | Train Loss: 0.3138639 Vali Loss: 0.2099251 Test Loss: 2.0225795 MAE Loss: 0.8939659
Updating learning rate to 1.2499999999999979e-05
	iters: 100, epoch: 7 | loss: 0.2204261
	speed: 2.3160s/iter; left time: 919.4547s
Epoch: 7 cost time: 205.44119954109192
Epoch: 7 | Train Loss: 0.3062167 Vali Loss: 0.2138709 Test Loss: 2.1613730 MAE Loss: 0.9223106
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.249999999999989e-06
	iters: 100, epoch: 8 | loss: 0.3310906
	speed: 2.3162s/iter; left time: 632.3198s
Epoch: 8 cost time: 205.45730304718018
Epoch: 8 | Train Loss: 0.3032132 Vali Loss: 0.2159047 Test Loss: 2.0520699 MAE Loss: 0.8985787
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1249999999999946e-06
	iters: 100, epoch: 9 | loss: 0.2714978
	speed: 2.3154s/iter; left time: 344.9961s
Epoch: 9 cost time: 205.54506635665894
Epoch: 9 | Train Loss: 0.2964108 Vali Loss: 0.2131617 Test Loss: 2.1090375 MAE Loss: 0.9087499
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.5624999999999973e-06
	iters: 100, epoch: 10 | loss: 0.3160454
	speed: 2.3159s/iter; left time: 57.8964s
Epoch: 10 cost time: 205.48405981063843
Epoch: 10 | Train Loss: 0.2984510 Vali Loss: 0.2086597 Test Loss: 2.1116410 MAE Loss: 0.9090590
Updating learning rate to 7.812499999999987e-07
success delete checkpoints
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
Gradient Checkpointing: True
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
[2025-03-31 20:12:03,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-31 20:12:03,920] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-03-31 20:12:03,920] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-03-31 20:12:03,921] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-03-31 20:12:04,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.51.187, master_port=29500
[2025-03-31 20:12:04,657] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-31 20:12:19,553] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-31 20:12:19,554] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-03-31 20:12:19,555] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-31 20:12:19,555] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-03-31 20:12:19,555] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-03-31 20:12:19,555] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-31 20:12:19,556] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-31 20:12:19,556] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-31 20:12:19,556] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-31 20:12:19,556] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-31 20:12:19,818] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2025-03-31 20:12:19,819] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.6 GB         CA 12.61 GB         Max_CA 13 GB 
[2025-03-31 20:12:19,819] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.09 GB, percent = 1.1%
[2025-03-31 20:12:19,928] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2025-03-31 20:12:19,928] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.67 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 20:12:19,928] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.09 GB, percent = 1.1%
[2025-03-31 20:12:19,928] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2025-03-31 20:12:20,033] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2025-03-31 20:12:20,033] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.54 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 20:12:20,033] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.09 GB, percent = 1.1%
[2025-03-31 20:12:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2025-03-31 20:12:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-03-31 20:12:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-03-31 20:12:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]
[2025-03-31 20:12:20,035] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-03-31 20:12:20,035] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2b98381e50>
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2025-03-31 20:12:20,036] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   world_size ................... 1
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2025-03-31 20:12:20,037] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-31 20:12:20,038] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2025-03-31 20:12:20,038] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-31 20:12:20,038] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2025-03-31 20:12:20,038] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.3856636
	speed: 1.8175s/iter; left time: 2037.4641s
Epoch: 1 cost time: 202.24662280082703
Epoch: 1 | Train Loss: 0.7547430 Vali Loss: 0.2440286 Test Loss: 2.2830743 MAE Loss: 0.9957247
lr = 0.0004000000
Updating learning rate to 0.0003999999999999993
	iters: 100, epoch: 2 | loss: 0.5227239
	speed: 2.2517s/iter; left time: 2249.4075s
Epoch: 2 cost time: 202.2824296951294
Epoch: 2 | Train Loss: 0.5266200 Vali Loss: 0.2153576 Test Loss: 2.1002356 MAE Loss: 0.9516959
Updating learning rate to 0.00019999999999999966
	iters: 100, epoch: 3 | loss: 0.3474576
	speed: 2.2504s/iter; left time: 1973.6221s
Epoch: 3 cost time: 202.00502729415894
Epoch: 3 | Train Loss: 0.4344515 Vali Loss: 0.2312555 Test Loss: 2.1533605 MAE Loss: 0.9756205
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.999999999999983e-05
	iters: 100, epoch: 4 | loss: 0.4941766
	speed: 2.2535s/iter; left time: 1701.3921s
Epoch: 4 cost time: 202.33747243881226
Epoch: 4 | Train Loss: 0.3958458 Vali Loss: 0.2251264 Test Loss: 2.1414015 MAE Loss: 0.9696752
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.9999999999999914e-05
	iters: 100, epoch: 5 | loss: 0.4244972
	speed: 2.2514s/iter; left time: 1425.1242s
Epoch: 5 cost time: 202.15288376808167
Epoch: 5 | Train Loss: 0.3606704 Vali Loss: 0.2138149 Test Loss: 1.9866102 MAE Loss: 0.9101150
Updating learning rate to 2.4999999999999957e-05
	iters: 100, epoch: 6 | loss: 0.4029417
	speed: 2.2499s/iter; left time: 1149.6747s
Epoch: 6 cost time: 202.0173852443695
Epoch: 6 | Train Loss: 0.3355133 Vali Loss: 0.2018184 Test Loss: 1.9294196 MAE Loss: 0.8910968
Updating learning rate to 1.2499999999999979e-05
	iters: 100, epoch: 7 | loss: 0.3961139
	speed: 2.2526s/iter; left time: 876.2508s
Epoch: 7 cost time: 202.05573558807373
Epoch: 7 | Train Loss: 0.3329869 Vali Loss: 0.2015782 Test Loss: 1.9657762 MAE Loss: 0.8926996
Updating learning rate to 6.249999999999989e-06
	iters: 100, epoch: 8 | loss: 0.4578589
	speed: 2.2512s/iter; left time: 601.0814s
Epoch: 8 cost time: 202.10898065567017
Epoch: 8 | Train Loss: 0.3267566 Vali Loss: 0.2041204 Test Loss: 2.0018075 MAE Loss: 0.9052999
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1249999999999946e-06
	iters: 100, epoch: 9 | loss: 0.3232947
	speed: 2.2493s/iter; left time: 326.1477s
Epoch: 9 cost time: 202.01646041870117
Epoch: 9 | Train Loss: 0.3259502 Vali Loss: 0.1950824 Test Loss: 1.9839167 MAE Loss: 0.8970686
Updating learning rate to 1.5624999999999973e-06
	iters: 100, epoch: 10 | loss: 0.3070323
	speed: 2.2488s/iter; left time: 51.7222s
Epoch: 10 cost time: 201.84604001045227
Epoch: 10 | Train Loss: 0.3241370 Vali Loss: 0.2010382 Test Loss: 2.0140094 MAE Loss: 0.9053841
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.812499999999987e-07
success delete checkpoints
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
Gradient Checkpointing: True
ILI includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients.
[2025-03-31 20:50:39,837] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-31 20:50:40,390] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-03-31 20:50:40,390] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-03-31 20:50:40,390] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-03-31 20:50:41,112] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.51.187, master_port=29500
[2025-03-31 20:50:41,112] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-31 20:50:54,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-31 20:50:54,078] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-03-31 20:50:54,078] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-31 20:50:54,080] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-03-31 20:50:54,080] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-03-31 20:50:54,080] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-03-31 20:50:54,081] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-31 20:50:54,081] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-31 20:50:54,081] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-31 20:50:54,081] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-31 20:50:54,363] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2025-03-31 20:50:54,364] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.6 GB         CA 12.61 GB         Max_CA 13 GB 
[2025-03-31 20:50:54,364] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.07 GB, percent = 1.1%
[2025-03-31 20:50:54,470] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2025-03-31 20:50:54,470] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.67 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 20:50:54,470] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.07 GB, percent = 1.1%
[2025-03-31 20:50:54,471] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2025-03-31 20:50:54,572] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2025-03-31 20:50:54,572] [INFO] [utils.py:801:see_memory_usage] MA 12.54 GB         Max_MA 12.54 GB         CA 12.74 GB         Max_CA 13 GB 
[2025-03-31 20:50:54,573] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 11.07 GB, percent = 1.1%
[2025-03-31 20:50:54,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2025-03-31 20:50:54,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-03-31 20:50:54,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-03-31 20:50:54,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003999999999999993], mom=[(0.95, 0.999)]
[2025-03-31 20:50:54,574] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-03-31 20:50:54,574] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-31 20:50:54,574] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-03-31 20:50:54,574] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-03-31 20:50:54,574] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-03-31 20:50:54,574] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2e1c6d68d0>
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-03-31 20:50:54,575] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-03-31 20:50:54,576] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   world_size ................... 1
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-31 20:50:54,577] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2025-03-31 20:50:54,577] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.7506301
	speed: 1.7994s/iter; left time: 1963.0921s
Epoch: 1 cost time: 197.57344722747803
Epoch: 1 | Train Loss: 0.8826793 Vali Loss: 0.6476710 Test Loss: 4.2165628 MAE Loss: 1.4945094
lr = 0.0004000000
Updating learning rate to 0.0003999999999999993
	iters: 100, epoch: 2 | loss: 0.7589097
	speed: 2.1823s/iter; left time: 2121.2325s
Epoch: 2 cost time: 197.2950258255005
Epoch: 2 | Train Loss: 0.8188920 Vali Loss: 0.3389211 Test Loss: 2.9087423 MAE Loss: 1.1666814
Updating learning rate to 0.00019999999999999966
	iters: 100, epoch: 3 | loss: 0.5740937
	speed: 2.1809s/iter; left time: 1860.2983s
Epoch: 3 cost time: 197.2526876926422
Epoch: 3 | Train Loss: 0.6833591 Vali Loss: 0.2315462 Test Loss: 2.6131222 MAE Loss: 1.0829323
Updating learning rate to 9.999999999999983e-05
	iters: 100, epoch: 4 | loss: 0.5847125
	speed: 2.1814s/iter; left time: 1601.1245s
Epoch: 4 cost time: 197.27861881256104
Epoch: 4 | Train Loss: 0.5948024 Vali Loss: 0.2571670 Test Loss: 2.5329319 MAE Loss: 1.0730185
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9999999999999914e-05
	iters: 100, epoch: 5 | loss: 0.3883772
	speed: 2.1839s/iter; left time: 1343.1221s
Epoch: 5 cost time: 197.31851649284363
Epoch: 5 | Train Loss: 0.5610426 Vali Loss: 0.2138900 Test Loss: 2.2678274 MAE Loss: 0.9978524
Updating learning rate to 2.4999999999999957e-05
	iters: 100, epoch: 6 | loss: 0.7351943
	speed: 2.1827s/iter; left time: 1082.6269s
Epoch: 6 cost time: 197.4259467124939
Epoch: 6 | Train Loss: 0.5457902 Vali Loss: 0.2275709 Test Loss: 2.3257420 MAE Loss: 1.0162938
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2499999999999979e-05
	iters: 100, epoch: 7 | loss: 0.8131090
	speed: 2.1814s/iter; left time: 822.3953s
Epoch: 7 cost time: 197.2076756954193
Epoch: 7 | Train Loss: 0.5378677 Vali Loss: 0.2426190 Test Loss: 2.4390686 MAE Loss: 1.0463413
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.249999999999989e-06
	iters: 100, epoch: 8 | loss: 0.6931698
	speed: 2.1814s/iter; left time: 562.8001s
Epoch: 8 cost time: 197.25477719306946
Epoch: 8 | Train Loss: 0.5337765 Vali Loss: 0.2305906 Test Loss: 2.2844828 MAE Loss: 1.0040629
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.1249999999999946e-06
	iters: 100, epoch: 9 | loss: 0.5120127
	speed: 2.1825s/iter; left time: 303.3693s
Epoch: 9 cost time: 197.19255685806274
Epoch: 9 | Train Loss: 0.5326255 Vali Loss: 0.2212900 Test Loss: 2.3139700 MAE Loss: 1.0117260
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5624999999999973e-06
	iters: 100, epoch: 10 | loss: 0.4538604
	speed: 2.1835s/iter; left time: 43.6704s
Epoch: 10 cost time: 197.41346907615662
Epoch: 10 | Train Loss: 0.5312814 Vali Loss: 0.2201452 Test Loss: 2.2858984 MAE Loss: 1.0039759
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.812499999999987e-07
success delete checkpoints
