wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Appending key for api.wandb.ai to your netrc file: /home/e/e1350606/.netrc
wandb: Currently logged in as: 3317701822 (3317701822-national-university-of-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/e/e1350606/time_ttl/Time-LLM/wandb/run-20250408_015244-4qni0dpg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-dust-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot
wandb: üöÄ View run at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot/runs/4qni0dpg
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:06<00:20,  6.83s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:14<00:14,  7.07s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:21<00:07,  7.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.48s/it]
0it [00:00, ?it/s]0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/e/e1350606/time_ttl/Time-LLM/run_main.py", line 264, in <module>
    accelerator.backward(loss)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 1995, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1976, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2051, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
Traceback (most recent call last):
  File "/home/e/e1350606/miniconda3/envs/qwen/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1057, in launch_command
    simple_launcher(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 673, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/e/e1350606/miniconda3/envs/qwen/bin/python', 'run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './dataset/ETT-small/', '--data_path', 'ETTh1.csv', '--model_id', 'ETTh1_512_192', '--model', 'TimeLLM', '--data', 'ETTh1', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '192', '--factor', '3', '--enc_in', '7', '--dec_in', '7', '--c_out', '7', '--des', 'Exp', '--itr', '1', '--d_model', '32', '--d_ff', '128', '--batch_size', '4', '--learning_rate', '0.02', '--llm_layers', '32', '--train_epochs', '10', '--model_comment', 'TimeLLM-ETTh1', '--window_size', '64', '--llm_model', 'DeepSeek', '--llm_dim', '2048', '--wandb_title', 'long-term-few-shot', '--percent', '10', '--prompt_domain', '1']' returned non-zero exit status 1.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Appending key for api.wandb.ai to your netrc file: /home/e/e1350606/.netrc
wandb: Currently logged in as: 3317701822 (3317701822-national-university-of-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/e/e1350606/time_ttl/Time-LLM/wandb/run-20250408_015405-6hm8g8tv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-deluge-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot
wandb: üöÄ View run at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot/runs/6hm8g8tv
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:06<00:20,  6.81s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:14<00:14,  7.07s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:21<00:07,  7.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.49s/it]
0it [00:00, ?it/s]1it [00:05,  5.50s/it]2it [00:06,  2.97s/it]3it [00:07,  2.16s/it]4it [00:09,  1.99s/it]5it [00:11,  1.89s/it]6it [00:12,  1.66s/it]7it [00:13,  1.51s/it]8it [00:14,  1.41s/it]9it [00:16,  1.35s/it]10it [00:17,  1.30s/it]11it [00:18,  1.27s/it]12it [00:19,  1.24s/it]13it [00:20,  1.23s/it]14it [00:22,  1.22s/it]15it [00:23,  1.37s/it]16it [00:25,  1.31s/it]17it [00:26,  1.27s/it]18it [00:27,  1.25s/it]19it [00:29,  1.39s/it]20it [00:30,  1.33s/it]21it [00:31,  1.28s/it]22it [00:32,  1.26s/it]23it [00:33,  1.24s/it]24it [00:35,  1.24s/it]25it [00:36,  1.24s/it]26it [00:37,  1.23s/it]27it [00:38,  1.29s/it]28it [00:40,  1.26s/it]29it [00:41,  1.25s/it]30it [00:42,  1.23s/it]31it [00:43,  1.23s/it]32it [00:45,  1.22s/it]33it [00:46,  1.21s/it]34it [00:47,  1.21s/it]35it [00:48,  1.20s/it]36it [00:49,  1.19s/it]37it [00:50,  1.19s/it]38it [00:52,  1.19s/it]39it [00:53,  1.20s/it]40it [00:54,  1.19s/it]41it [00:55,  1.18s/it]42it [00:56,  1.19s/it]43it [00:58,  1.19s/it]44it [00:59,  1.20s/it]45it [01:00,  1.20s/it]46it [01:02,  1.35s/it]47it [01:03,  1.31s/it]48it [01:04,  1.28s/it]49it [01:05,  1.26s/it]50it [01:07,  1.24s/it]51it [01:08,  1.22s/it]52it [01:09,  1.21s/it]53it [01:10,  1.20s/it]54it [01:11,  1.20s/it]55it [01:13,  1.25s/it]56it [01:14,  1.39s/it]57it [01:16,  1.32s/it]58it [01:17,  1.28s/it]59it [01:18,  1.26s/it]60it [01:19,  1.24s/it]61it [01:20,  1.23s/it]62it [01:22,  1.22s/it]63it [01:23,  1.22s/it]64it [01:24,  1.21s/it]65it [01:25,  1.21s/it]66it [01:26,  1.22s/it]67it [01:28,  1.21s/it]68it [01:29,  1.20s/it]69it [01:30,  1.20s/it]70it [01:31,  1.19s/it]71it [01:32,  1.20s/it]72it [01:34,  1.20s/it]73it [01:35,  1.33s/it]74it [01:36,  1.30s/it]75it [01:38,  1.26s/it]76it [01:39,  1.24s/it]77it [01:40,  1.37s/it]78it [01:42,  1.32s/it]79it [01:43,  1.29s/it]80it [01:44,  1.26s/it]81it [01:45,  1.25s/it]82it [01:46,  1.23s/it]83it [01:48,  1.23s/it]84it [01:49,  1.27s/it]85it [01:50,  1.25s/it]86it [01:52,  1.40s/it]87it [01:54,  1.50s/it]88it [01:55,  1.41s/it]89it [01:56,  1.35s/it]90it [01:57,  1.31s/it]91it [01:59,  1.28s/it]92it [02:00,  1.24s/it]93it [02:01,  1.23s/it]94it [02:02,  1.22s/it]95it [02:03,  1.21s/it]96it [02:05,  1.21s/it]97it [02:06,  1.21s/it]98it [02:07,  1.20s/it]99it [02:08,  1.20s/it]100it [02:09,  1.21s/it]101it [02:11,  1.20s/it]102it [02:12,  1.19s/it]103it [02:13,  1.20s/it]104it [02:15,  1.35s/it]105it [02:16,  1.31s/it]106it [02:18,  1.44s/it]107it [02:19,  1.37s/it]108it [02:20,  1.31s/it]109it [02:21,  1.34s/it]110it [02:23,  1.31s/it]111it [02:24,  1.27s/it]112it [02:25,  1.25s/it]113it [02:26,  1.24s/it]114it [02:27,  1.22s/it]115it [02:29,  1.21s/it]116it [02:30,  1.20s/it]117it [02:31,  1.19s/it]118it [02:32,  1.20s/it]119it [02:33,  1.19s/it]120it [02:34,  1.19s/it]121it [02:36,  1.19s/it]122it [02:37,  1.18s/it]123it [02:38,  1.19s/it]124it [02:39,  1.18s/it]125it [02:40,  1.19s/it]126it [02:42,  1.20s/it]127it [02:43,  1.20s/it]128it [02:44,  1.20s/it]129it [02:45,  1.20s/it]130it [02:46,  1.20s/it]131it [02:48,  1.21s/it]132it [02:49,  1.21s/it]133it [02:50,  1.21s/it]134it [02:51,  1.21s/it]135it [02:52,  1.20s/it]136it [02:54,  1.25s/it]137it [02:55,  1.24s/it]138it [02:56,  1.22s/it]139it [02:57,  1.20s/it]140it [02:59,  1.20s/it]141it [03:00,  1.19s/it]142it [03:01,  1.19s/it]143it [03:02,  1.19s/it]144it [03:03,  1.19s/it]145it [03:05,  1.20s/it]146it [03:06,  1.20s/it]147it [03:07,  1.20s/it]148it [03:08,  1.20s/it]149it [03:09,  1.21s/it]150it [03:11,  1.20s/it]151it [03:12,  1.19s/it]152it [03:13,  1.19s/it]153it [03:14,  1.20s/it]154it [03:15,  1.19s/it]155it [03:17,  1.20s/it]156it [03:18,  1.20s/it]157it [03:19,  1.20s/it]158it [03:20,  1.21s/it]159it [03:21,  1.20s/it]160it [03:23,  1.20s/it]161it [03:24,  1.20s/it]162it [03:27,  1.68s/it]162it [03:27,  1.28s/it]
0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/e/e1350606/time_ttl/Time-LLM/run_main.py", line 275, in <module>
    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/time_ttl/Time-LLM/utils/tools.py", line 166, in vali
    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2242, in gather_for_metrics
    data = self.gather(input_data)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2205, in gather
    return gather(tensor)
           ^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 378, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 439, in gather
    return _gpu_gather(tensor)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 358, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 107, in recursively_apply
    return honor_type(
           ^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 81, in honor_type
    return type(obj)(generator)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 110, in <genexpr>
    recursively_apply(
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 355, in _gpu_gather_one
    torch.distributed.all_gather(output_tensors, tensor)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2615, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Failed to CUDA calloc async 4 bytes
Traceback (most recent call last):
  File "/home/e/e1350606/miniconda3/envs/qwen/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1057, in launch_command
    simple_launcher(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 673, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/e/e1350606/miniconda3/envs/qwen/bin/python', 'run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './dataset/ETT-small/', '--data_path', 'ETTh1.csv', '--model_id', 'ETTh1_512_720', '--model', 'TimeLLM', '--data', 'ETTh1', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '720', '--factor', '3', '--enc_in', '7', '--dec_in', '7', '--c_out', '7', '--des', 'Exp', '--itr', '1', '--d_model', '32', '--d_ff', '128', '--batch_size', '4', '--learning_rate', '0.01', '--llm_layers', '32', '--train_epochs', '10', '--model_comment', 'TimeLLM-ETTh1', '--window_size', '64', '--llm_model', 'DeepSeek', '--llm_dim', '2048', '--wandb_title', 'long-term-few-shot', '--percent', '10', '--prompt_domain', '1']' returned non-zero exit status 1.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Appending key for api.wandb.ai to your netrc file: /home/e/e1350606/.netrc
wandb: Currently logged in as: 3317701822 (3317701822-national-university-of-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/e/e1350606/time_ttl/Time-LLM/wandb/run-20250408_015851-jm764pcr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-capybara-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot
wandb: üöÄ View run at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot/runs/jm764pcr
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:06<00:20,  6.74s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:13<00:13,  6.93s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:20<00:06,  6.97s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.35s/it]
0it [00:00, ?it/s]1it [00:06,  6.10s/it]2it [00:07,  3.20s/it]3it [00:08,  2.27s/it]4it [00:09,  1.82s/it]5it [00:10,  1.58s/it]6it [00:11,  1.44s/it]7it [00:13,  1.34s/it]8it [00:14,  1.29s/it]9it [00:15,  1.24s/it]10it [00:16,  1.36s/it]11it [00:18,  1.29s/it]12it [00:19,  1.25s/it]13it [00:20,  1.38s/it]14it [00:22,  1.32s/it]15it [00:23,  1.27s/it]16it [00:24,  1.22s/it]17it [00:25,  1.20s/it]18it [00:26,  1.19s/it]19it [00:27,  1.17s/it]20it [00:28,  1.17s/it]21it [00:30,  1.17s/it]22it [00:31,  1.16s/it]23it [00:32,  1.16s/it]24it [00:33,  1.17s/it]25it [00:34,  1.16s/it]26it [00:35,  1.16s/it]27it [00:37,  1.23s/it]28it [00:38,  1.21s/it]29it [00:39,  1.20s/it]30it [00:40,  1.18s/it]31it [00:41,  1.18s/it]32it [00:43,  1.18s/it]33it [00:44,  1.18s/it]34it [00:45,  1.17s/it]35it [00:46,  1.16s/it]36it [00:47,  1.16s/it]37it [00:48,  1.16s/it]38it [00:50,  1.15s/it]39it [00:51,  1.16s/it]40it [00:52,  1.17s/it]41it [00:53,  1.17s/it]42it [00:55,  1.34s/it]43it [00:56,  1.29s/it]44it [00:57,  1.25s/it]45it [00:58,  1.23s/it]46it [01:00,  1.38s/it]47it [01:02,  1.49s/it]48it [01:03,  1.40s/it]49it [01:04,  1.33s/it]50it [01:05,  1.27s/it]51it [01:06,  1.23s/it]52it [01:08,  1.22s/it]53it [01:09,  1.20s/it]54it [01:10,  1.20s/it]55it [01:11,  1.22s/it]56it [01:12,  1.21s/it]57it [01:14,  1.20s/it]58it [01:15,  1.18s/it]59it [01:16,  1.18s/it]60it [01:18,  1.34s/it]61it [01:19,  1.45s/it]62it [01:21,  1.53s/it]63it [01:22,  1.43s/it]64it [01:24,  1.52s/it]65it [01:25,  1.42s/it]66it [01:26,  1.34s/it]67it [01:28,  1.28s/it]68it [01:29,  1.25s/it]69it [01:30,  1.23s/it]70it [01:31,  1.20s/it]71it [01:32,  1.18s/it]72it [01:33,  1.18s/it]73it [01:34,  1.18s/it]74it [01:36,  1.17s/it]75it [01:37,  1.17s/it]76it [01:38,  1.16s/it]77it [01:39,  1.15s/it]78it [01:40,  1.16s/it]79it [01:41,  1.16s/it]80it [01:43,  1.17s/it]81it [01:44,  1.16s/it]82it [01:45,  1.15s/it]83it [01:46,  1.16s/it]84it [01:47,  1.20s/it]85it [01:49,  1.19s/it]86it [01:50,  1.18s/it]87it [01:51,  1.18s/it]88it [01:52,  1.18s/it]89it [01:53,  1.18s/it]90it [01:54,  1.18s/it]91it [01:56,  1.17s/it]92it [01:57,  1.17s/it]93it [01:58,  1.17s/it]94it [01:59,  1.18s/it]95it [02:01,  1.33s/it]96it [02:02,  1.29s/it]97it [02:04,  1.42s/it]98it [02:05,  1.52s/it]99it [02:07,  1.42s/it]100it [02:08,  1.34s/it]101it [02:09,  1.29s/it]102it [02:10,  1.25s/it]103it [02:12,  1.38s/it]104it [02:13,  1.32s/it]105it [02:14,  1.28s/it]106it [02:15,  1.25s/it]107it [02:17,  1.22s/it]108it [02:18,  1.20s/it]109it [02:20,  1.41s/it]110it [02:21,  1.34s/it]111it [02:22,  1.28s/it]112it [02:23,  1.24s/it]113it [02:24,  1.21s/it]114it [02:25,  1.20s/it]115it [02:27,  1.19s/it]116it [02:28,  1.19s/it]117it [02:29,  1.18s/it]118it [02:30,  1.18s/it]119it [02:31,  1.17s/it]120it [02:32,  1.16s/it]121it [02:33,  1.16s/it]122it [02:35,  1.17s/it]123it [02:36,  1.17s/it]124it [02:37,  1.17s/it]125it [02:38,  1.16s/it]126it [02:39,  1.16s/it]127it [02:40,  1.16s/it]128it [02:42,  1.16s/it]129it [02:43,  1.16s/it]130it [02:44,  1.15s/it]131it [02:45,  1.15s/it]132it [02:47,  1.31s/it]133it [02:48,  1.27s/it]134it [02:49,  1.23s/it]135it [02:50,  1.22s/it]136it [02:52,  1.41s/it]137it [02:53,  1.34s/it]138it [02:54,  1.28s/it]139it [02:56,  1.25s/it]140it [02:57,  1.23s/it]141it [02:58,  1.21s/it]142it [02:59,  1.20s/it]143it [03:00,  1.19s/it]144it [03:01,  1.19s/it]145it [03:03,  1.17s/it]146it [03:04,  1.16s/it]147it [03:05,  1.17s/it]148it [03:07,  1.34s/it]149it [03:08,  1.30s/it]150it [03:09,  1.26s/it]151it [03:11,  1.39s/it]152it [03:12,  1.32s/it]153it [03:13,  1.28s/it]154it [03:14,  1.24s/it]155it [03:15,  1.22s/it]156it [03:17,  1.38s/it]157it [03:18,  1.32s/it]158it [03:20,  1.28s/it]159it [03:21,  1.25s/it]160it [03:22,  1.22s/it]161it [03:23,  1.20s/it]162it [03:24,  1.20s/it]163it [03:26,  1.24s/it]164it [03:27,  1.21s/it]165it [03:28,  1.19s/it]166it [03:29,  1.17s/it]167it [03:30,  1.17s/it]168it [03:31,  1.17s/it]169it [03:32,  1.17s/it]170it [03:34,  1.17s/it]171it [03:35,  1.18s/it]172it [03:36,  1.18s/it]173it [03:37,  1.18s/it]174it [03:38,  1.17s/it]175it [03:39,  1.16s/it]176it [03:41,  1.16s/it]177it [03:42,  1.17s/it]178it [03:44,  1.34s/it]179it [03:45,  1.30s/it]180it [03:46,  1.25s/it]181it [03:47,  1.23s/it]182it [03:48,  1.21s/it]183it [03:49,  1.20s/it]184it [03:51,  1.20s/it]185it [03:52,  1.19s/it]186it [03:53,  1.18s/it]187it [03:54,  1.18s/it]188it [03:55,  1.17s/it]189it [03:56,  1.17s/it]190it [03:58,  1.16s/it]191it [03:59,  1.21s/it]192it [04:00,  1.19s/it]193it [04:01,  1.18s/it]194it [04:02,  1.18s/it]195it [04:04,  1.17s/it]196it [04:05,  1.17s/it]197it [04:06,  1.17s/it]198it [04:07,  1.18s/it]199it [04:08,  1.17s/it]200it [04:09,  1.16s/it]201it [04:10,  1.15s/it]202it [04:12,  1.32s/it]203it [04:13,  1.27s/it]204it [04:15,  1.23s/it]205it [04:16,  1.21s/it]206it [04:17,  1.20s/it]207it [04:18,  1.19s/it]208it [04:19,  1.18s/it]209it [04:20,  1.18s/it]210it [04:21,  1.17s/it]211it [04:23,  1.16s/it]212it [04:24,  1.16s/it]213it [04:25,  1.16s/it]214it [04:26,  1.16s/it]215it [04:27,  1.15s/it]216it [04:28,  1.16s/it]217it [04:30,  1.16s/it]218it [04:31,  1.17s/it]219it [04:32,  1.17s/it]220it [04:34,  1.34s/it]221it [04:36,  1.51s/it]222it [04:37,  1.41s/it]223it [04:38,  1.34s/it]224it [04:39,  1.29s/it]225it [04:40,  1.26s/it]226it [04:42,  1.24s/it]227it [04:43,  1.21s/it]228it [04:44,  1.20s/it]229it [04:45,  1.20s/it]230it [04:46,  1.18s/it]231it [04:47,  1.18s/it]232it [04:48,  1.16s/it]233it [04:50,  1.17s/it]234it [04:51,  1.33s/it]235it [04:52,  1.28s/it]236it [04:54,  1.41s/it]237it [04:55,  1.34s/it]238it [04:57,  1.29s/it]239it [04:58,  1.26s/it]240it [04:59,  1.23s/it]241it [05:00,  1.22s/it]242it [05:02,  1.37s/it]243it [05:03,  1.30s/it]244it [05:04,  1.26s/it]245it [05:05,  1.23s/it]246it [05:07,  1.42s/it]247it [05:08,  1.34s/it]248it [05:09,  1.28s/it]249it [05:11,  1.25s/it]250it [05:12,  1.23s/it]251it [05:13,  1.20s/it]252it [05:14,  1.18s/it]253it [05:15,  1.18s/it]254it [05:17,  1.35s/it]255it [05:18,  1.30s/it]256it [05:19,  1.25s/it]257it [05:20,  1.22s/it]258it [05:22,  1.20s/it]259it [05:23,  1.20s/it]260it [05:24,  1.19s/it]261it [05:25,  1.18s/it]262it [05:26,  1.17s/it]263it [05:27,  1.17s/it]264it [05:29,  1.16s/it]265it [05:30,  1.17s/it]266it [05:31,  1.18s/it]267it [05:32,  1.17s/it]268it [05:33,  1.18s/it]269it [05:35,  1.17s/it]270it [05:36,  1.17s/it]271it [05:37,  1.16s/it]272it [05:38,  1.17s/it]273it [05:39,  1.22s/it]274it [05:40,  1.20s/it]275it [05:42,  1.19s/it]276it [05:43,  1.19s/it]277it [05:44,  1.18s/it]278it [05:45,  1.18s/it]279it [05:46,  1.18s/it]280it [05:48,  1.18s/it]281it [05:49,  1.18s/it]282it [05:50,  1.17s/it]283it [05:51,  1.17s/it]284it [05:52,  1.17s/it]285it [05:53,  1.17s/it]286it [05:55,  1.17s/it]287it [05:56,  1.16s/it]288it [05:57,  1.16s/it]289it [05:58,  1.16s/it]290it [05:59,  1.17s/it]291it [06:00,  1.17s/it]292it [06:02,  1.17s/it]293it [06:03,  1.17s/it]294it [06:04,  1.16s/it]295it [06:06,  1.33s/it]296it [06:07,  1.29s/it]297it [06:08,  1.25s/it]298it [06:09,  1.23s/it]299it [06:10,  1.21s/it]300it [06:12,  1.25s/it]301it [06:13,  1.23s/it]302it [06:14,  1.21s/it]303it [06:15,  1.20s/it]304it [06:16,  1.20s/it]305it [06:18,  1.19s/it]306it [06:19,  1.19s/it]307it [06:20,  1.19s/it]308it [06:21,  1.17s/it]309it [06:22,  1.17s/it]310it [06:23,  1.17s/it]311it [06:25,  1.17s/it]312it [06:26,  1.17s/it]313it [06:27,  1.17s/it]314it [06:28,  1.16s/it]315it [06:30,  1.43s/it]316it [06:31,  1.35s/it]317it [06:32,  1.29s/it]318it [06:34,  1.26s/it]319it [06:35,  1.39s/it]320it [06:36,  1.32s/it]321it [06:38,  1.28s/it]322it [06:39,  1.25s/it]323it [06:40,  1.23s/it]324it [06:41,  1.21s/it]325it [06:42,  1.19s/it]326it [06:43,  1.18s/it]327it [06:45,  1.18s/it]328it [06:46,  1.23s/it]329it [06:47,  1.21s/it]330it [06:48,  1.19s/it]331it [06:49,  1.17s/it]332it [06:51,  1.17s/it]333it [06:52,  1.17s/it]334it [06:53,  1.17s/it]335it [06:54,  1.17s/it]336it [06:55,  1.17s/it]337it [06:56,  1.17s/it]338it [06:58,  1.34s/it]339it [07:00,  1.46s/it]340it [07:01,  1.38s/it]341it [07:02,  1.31s/it]342it [07:03,  1.27s/it]343it [07:05,  1.24s/it]344it [07:06,  1.22s/it]345it [07:07,  1.21s/it]346it [07:08,  1.20s/it]347it [07:09,  1.19s/it]348it [07:10,  1.19s/it]349it [07:12,  1.18s/it]350it [07:13,  1.18s/it]351it [07:14,  1.18s/it]352it [07:15,  1.17s/it]353it [07:16,  1.17s/it]354it [07:18,  1.17s/it]355it [07:19,  1.16s/it]356it [07:20,  1.17s/it]357it [07:21,  1.17s/it]358it [07:22,  1.21s/it]359it [07:23,  1.20s/it]360it [07:25,  1.19s/it]361it [07:26,  1.18s/it]362it [07:28,  1.34s/it]363it [07:29,  1.29s/it]364it [07:30,  1.42s/it]365it [07:32,  1.35s/it]366it [07:33,  1.29s/it]367it [07:34,  1.26s/it]368it [07:35,  1.23s/it]369it [07:37,  1.38s/it]370it [07:38,  1.32s/it]371it [07:39,  1.28s/it]372it [07:40,  1.24s/it]373it [07:42,  1.37s/it]374it [07:43,  1.31s/it]375it [07:44,  1.27s/it]376it [07:49,  2.33s/it]376it [07:49,  1.25s/it]
0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/e/e1350606/time_ttl/Time-LLM/run_main.py", line 275, in <module>
    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/time_ttl/Time-LLM/utils/tools.py", line 166, in vali
    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2242, in gather_for_metrics
    data = self.gather(input_data)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2205, in gather
    return gather(tensor)
           ^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 378, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 439, in gather
    return _gpu_gather(tensor)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 358, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 107, in recursively_apply
    return honor_type(
           ^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 81, in honor_type
    return type(obj)(generator)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 110, in <genexpr>
    recursively_apply(
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 355, in _gpu_gather_one
    torch.distributed.all_gather(output_tensors, tensor)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2615, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Failed to CUDA calloc async 4 bytes
Traceback (most recent call last):
  File "/home/e/e1350606/miniconda3/envs/qwen/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1057, in launch_command
    simple_launcher(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 673, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/e/e1350606/miniconda3/envs/qwen/bin/python', 'run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './dataset/ETT-small/', '--data_path', 'ETTh1.csv', '--model_id', 'ETTh1_512_192', '--model', 'TimeLLM', '--data', 'ETTh1', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '192', '--factor', '3', '--enc_in', '7', '--dec_in', '7', '--c_out', '7', '--des', 'Exp', '--itr', '1', '--d_model', '32', '--d_ff', '128', '--batch_size', '4', '--learning_rate', '0.02', '--llm_layers', '32', '--train_epochs', '10', '--model_comment', 'TimeLLM-ETTh1', '--window_size', '64', '--llm_model', 'DeepSeek', '--llm_dim', '2048', '--wandb_title', 'long-term-few-shot', '--percent', '5', '--prompt_domain', '1']' returned non-zero exit status 1.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Appending key for api.wandb.ai to your netrc file: /home/e/e1350606/.netrc
wandb: Currently logged in as: 3317701822 (3317701822-national-university-of-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/e/e1350606/time_ttl/Time-LLM/wandb/run-20250408_020746-xece3jgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-bird-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot
wandb: üöÄ View run at https://wandb.ai/3317701822-national-university-of-singapore/final-long-term-few-shot/runs/xece3jgz
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:06<00:20,  6.79s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:13<00:13,  6.99s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:20<00:07,  7.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:25<00:00,  6.40s/it]
0it [00:00, ?it/s]1it [00:05,  5.94s/it]2it [00:07,  3.15s/it]3it [00:08,  2.48s/it]4it [00:10,  1.97s/it]5it [00:11,  1.68s/it]6it [00:12,  1.51s/it]7it [00:13,  1.39s/it]8it [00:15,  1.50s/it]9it [00:16,  1.40s/it]10it [00:17,  1.32s/it]11it [00:18,  1.26s/it]12it [00:19,  1.23s/it]13it [00:21,  1.21s/it]14it [00:22,  1.19s/it]15it [00:23,  1.18s/it]16it [00:24,  1.17s/it]17it [00:25,  1.17s/it]18it [00:26,  1.16s/it]19it [00:27,  1.16s/it]20it [00:29,  1.15s/it]21it [00:30,  1.15s/it]22it [00:31,  1.16s/it]23it [00:32,  1.16s/it]24it [00:33,  1.15s/it]25it [00:34,  1.15s/it]26it [00:35,  1.15s/it]27it [00:37,  1.22s/it]28it [00:38,  1.19s/it]29it [00:39,  1.19s/it]30it [00:40,  1.18s/it]31it [00:41,  1.17s/it]32it [00:43,  1.34s/it]33it [00:44,  1.29s/it]34it [00:46,  1.26s/it]35it [00:47,  1.22s/it]36it [00:48,  1.20s/it]37it [00:49,  1.19s/it]38it [00:50,  1.18s/it]39it [00:51,  1.18s/it]40it [00:53,  1.17s/it]41it [00:54,  1.18s/it]42it [00:55,  1.17s/it]43it [00:56,  1.17s/it]44it [00:57,  1.18s/it]45it [00:58,  1.16s/it]46it [01:00,  1.17s/it]47it [01:01,  1.16s/it]48it [01:02,  1.16s/it]49it [01:03,  1.15s/it]50it [01:04,  1.15s/it]51it [01:05,  1.16s/it]52it [01:07,  1.32s/it]53it [01:08,  1.26s/it]54it [01:09,  1.24s/it]55it [01:11,  1.26s/it]56it [01:12,  1.24s/it]57it [01:13,  1.38s/it]58it [01:15,  1.49s/it]59it [01:16,  1.39s/it]60it [01:18,  1.33s/it]61it [01:19,  1.45s/it]62it [01:20,  1.37s/it]63it [01:22,  1.31s/it]64it [01:23,  1.26s/it]65it [01:24,  1.23s/it]66it [01:26,  1.37s/it]67it [01:27,  1.31s/it]68it [01:28,  1.26s/it]69it [01:29,  1.23s/it]70it [01:30,  1.21s/it]71it [01:31,  1.20s/it]72it [01:33,  1.19s/it]73it [01:34,  1.19s/it]74it [01:35,  1.18s/it]75it [01:37,  1.33s/it]76it [01:38,  1.29s/it]77it [01:39,  1.24s/it]78it [01:40,  1.21s/it]79it [01:41,  1.20s/it]80it [01:42,  1.18s/it]81it [01:44,  1.18s/it]82it [01:45,  1.18s/it]83it [01:46,  1.18s/it]84it [01:47,  1.21s/it]85it [01:48,  1.19s/it]86it [01:50,  1.19s/it]87it [01:51,  1.18s/it]88it [01:52,  1.18s/it]89it [01:53,  1.17s/it]90it [01:54,  1.17s/it]91it [01:55,  1.17s/it]92it [01:57,  1.17s/it]93it [01:58,  1.17s/it]94it [01:59,  1.17s/it]95it [02:00,  1.17s/it]96it [02:01,  1.16s/it]97it [02:02,  1.17s/it]98it [02:04,  1.18s/it]99it [02:05,  1.17s/it]100it [02:06,  1.17s/it]101it [02:07,  1.17s/it]102it [02:08,  1.17s/it]103it [02:09,  1.17s/it]104it [02:11,  1.17s/it]105it [02:12,  1.16s/it]106it [02:13,  1.17s/it]107it [02:14,  1.16s/it]108it [02:15,  1.16s/it]109it [02:17,  1.22s/it]110it [02:18,  1.21s/it]111it [02:19,  1.20s/it]112it [02:20,  1.18s/it]113it [02:21,  1.18s/it]114it [02:22,  1.18s/it]115it [02:24,  1.18s/it]116it [02:25,  1.18s/it]117it [02:26,  1.17s/it]118it [02:27,  1.16s/it]119it [02:28,  1.16s/it]120it [02:29,  1.17s/it]121it [02:31,  1.17s/it]122it [02:32,  1.17s/it]123it [02:33,  1.17s/it]124it [02:36,  1.68s/it]124it [02:36,  1.26s/it]
0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/e/e1350606/time_ttl/Time-LLM/run_main.py", line 275, in <module>
    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/time_ttl/Time-LLM/utils/tools.py", line 166, in vali
    outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2242, in gather_for_metrics
    data = self.gather(input_data)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/accelerator.py", line 2205, in gather
    return gather(tensor)
           ^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 378, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 439, in gather
    return _gpu_gather(tensor)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 358, in _gpu_gather
    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 107, in recursively_apply
    return honor_type(
           ^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 81, in honor_type
    return type(obj)(generator)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 110, in <genexpr>
    recursively_apply(
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/utils/operations.py", line 355, in _gpu_gather_one
    torch.distributed.all_gather(output_tensors, tensor)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2615, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1691, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.19.3
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Failed to CUDA calloc async 4 bytes
Traceback (most recent call last):
  File "/home/e/e1350606/miniconda3/envs/qwen/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1057, in launch_command
    simple_launcher(args)
  File "/home/e/e1350606/miniconda3/envs/qwen/lib/python3.11/site-packages/accelerate/commands/launch.py", line 673, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/e/e1350606/miniconda3/envs/qwen/bin/python', 'run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './dataset/ETT-small/', '--data_path', 'ETTh1.csv', '--model_id', 'ETTh1_512_336', '--model', 'TimeLLM', '--data', 'ETTh1', '--features', 'M', '--seq_len', '512', '--label_len', '48', '--pred_len', '336', '--factor', '3', '--enc_in', '7', '--dec_in', '7', '--c_out', '7', '--des', 'Exp', '--itr', '1', '--d_model', '32', '--d_ff', '128', '--batch_size', '4', '--lradj', 'COS', '--learning_rate', '0.001', '--llm_layers', '32', '--train_epochs', '10', '--model_comment', 'TimeLLM-ETTh1', '--window_size', '64', '--llm_model', 'DeepSeek', '--llm_dim', '2048', '--wandb_title', 'long-term-few-shot', '--percent', '5', '--prompt_domain', '1']' returned non-zero exit status 1.
